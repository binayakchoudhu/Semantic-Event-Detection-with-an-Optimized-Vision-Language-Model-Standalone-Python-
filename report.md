Semantic Event Detection with Optimized Vision-Language Model
Technical Report
1. Introduction
This report describes a Python pipeline for detecting semantic events in video using a Vision-Language Model (VLM), with model optimization techniques for real-time inference on resource-limited systems.
Target Events:
Person walking
Vehicle stopping  
Crowded scene
And other configurable semantic events
2. Model Selection
Chosen Model: CLIP (ViT-B/32)
Model: openai/clip-vit-base-patch32
Why CLIP?
| Criteria | CLIP Advantage |
|----------|----------------|
| Zero-shot capability | No training required for new events |
| Lightweight | ViT-B/32 has ~151M parameters |
| Multimodal | Directly compares images with text |
| Well-documented | Extensive HuggingFace support |
| Proven accuracy | Strong performance on diverse tasks |
Architecture:
Vision Encoder: Vision Transformer (ViT-B/32)
Text Encoder: Transformer-based
Joint embedding space for image-text similarity
How it works:
Extract video frames at configurable sample rate
Encode frames using CLIP's vision encoder
Compare frame embeddings with pre-computed text embeddings of event descriptions
Use softmax to get probability distribution over events
3. Optimization Techniques
3.1 Dynamic Quantization (INT8)
Technique: PyTorch Dynamic Quantization
What it does:
Converts FP32 weights to INT8 (8-bit integers)
Computes activations dynamically during inference
Targets Linear layers (dense layers)
Implementation:
quantized_model = torch.quantization.quantize_dynamic(
    model,
    {nn.Linear},
    dtype=torch.qint8
)
Expected Results:
~2-4x reduction in model size
~1.5-2x speedup on CPU
Minimal accuracy degradation (<1%)
3.2 Weight Pruning
Technique: L1 Unstructured Pruning
What it does:
Removes weights with smallest L1 magnitude
Creates sparse weight matrices
Configurable sparsity level (default: 30%)
Implementation:
prune.l1_unstructured(module, name='weight', amount=0.3)
Expected Results:
30% of weights set to zero
Slight speedup with sparse operations
Moderate accuracy impact at high sparsity
4. Performance Comparison
Actual Benchmark Results (MacOS, Apple Silicon, CPU)
| Model Variant | Size (MB) | Inference Time | FPS | Speedup |
|---------------|-----------|----------------|-----|---------|
| Original (FP32) | 577 | 27.90ms | 35.8 | 1.00x |
| Quantized (FP16)* | 289 | - | - | - |
| Pruned (30%) | 577 | 27.43ms | 36.5 | 1.02x |
*Note: INT8 quantization not supported on Apple Silicon; FP16 used instead.
Optimization Statistics
| Metric | Original | Pruned |
|--------|----------|--------|
| Total Parameters | 151,277,313 | 151,277,313 |
| Non-zero Parameters | 151,277,048 | 114,262,148 |
| Sparsity | 0% | 24.47% |
| Parameters Removed | - | 37,015,165 |
Metrics Measured
Inference Latency: Time per frame (milliseconds)
Throughput: Frames per second (FPS)
Model Size: Memory footprint (MB)
Parameter Count: Total vs non-zero parameters
5. Trade-offs Analysis
Quantization Trade-offs
| Aspect | Impact |
|--------|--------|
| Pros | Significant size reduction, faster CPU inference |
| Cons | Slight accuracy drop, GPU benefits limited |
| Best for | CPU deployment, edge devices |
Pruning Trade-offs
| Aspect | Impact |
|--------|--------|
| Pros | Maintains accuracy at low sparsity |
| Cons | Requires sparse runtime for speedup |
| Best for | Fine-tuning efficiency, combined with quantization |
6. Usage Guide
Installation
cd semantic_event_detection
pip install -r requirements.txt
Commands
Detect events in video:
python main.py detect path/to/video.mp4 --sample-rate 5 --max-frames 100
Optimize model:
python main.py optimize --output-dir ./optimized_models --prune-amount 0.3
Benchmark performance:
python main.py benchmark path/to/video.mp4 --num-frames 50
Run full pipeline:
python main.py full path/to/video.mp4 --output-dir ./results
7. Conclusions
CLIP is effective for zero-shot semantic event detection without task-specific training
Dynamic quantization provides the best trade-off between speedup and accuracy for CPU deployment
Pruning is most effective when combined with fine-tuning or when using sparse computation libraries
For real-time performance (30+ FPS), consider:
Using GPU acceleration
Lower resolution input
Higher frame sampling rate
Smaller CLIP variants (ViT-B/16 â†’ ViT-S)
8. Future Improvements
Implement ONNX export for broader deployment
Add TensorRT optimization for NVIDIA GPUs
Explore knowledge distillation to smaller models
Implement temporal smoothing for event detection
Add support for custom event fine-tuning
***Author: BINAYAK CHOUDHURY  
Date: February 2026  
License: MIT
